Title: AI Bot Audio to audio


Table of Contents


Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#table-of-contents
- Application Architecture
Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#application-architecture
- Installation
Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#installation
- Running the Project
Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#running-the-project
- File Descriptions
Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#file-descriptions

Application Architecture


Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#application-architecture
Below is the overview of the architecture for an AI audio-to-audio chatbot application leveraging OpenAI Whisper and ElevenLabs’ text-to-speech (TTS) API:
Link: https://private-user-images.githubusercontent.com/29508011/302569766-bef9ed29-cbce-4d4c-a83b-7f2ee44379bc.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTIxMjg0NjMsIm5iZiI6MTcxMjEyODE2MywicGF0aCI6Ii8yOTUwODAxMS8zMDI1Njk3NjYtYmVmOWVkMjktY2JjZS00ZDRjLWE4M2ItN2YyZWU0NDM3OWJjLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MDMlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDAzVDA3MDkyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI1ZjQyMzhiMWQ0ZTg5ZmExNmQ3NWI0YzExZTgyNWUyZGUxOTU3ODUxMmFmOTkzMTI2MmVjN2RjYTQ1ZjNkOWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.JnMHu0vQUK6z7ksbw8zKWs7RqI-OyTlnKfmXEtUeks8
- Physical Input (Voice Recording):The user provides a spoken input through a microphone.

PyAudio library is used to record the audio input, saving it as a .wav file.
- The user provides a spoken input through a microphone.
- PyAudio library is used to record the audio input, saving it as a .wav file.
- Transcribe:The recorded .wav file is fed into OpenAI Whisper for speech-to-text transcription.
- The recorded .wav file is fed into OpenAI Whisper for speech-to-text transcription.
- GPT-3.5 Turbo:The transcribed text is passed to GPT-3.5 Turbo, a large language model fine-tuned for various tasks.

GPT-3.5 Turbo generates a response text based on the input.
- The transcribed text is passed to GPT-3.5 Turbo, a large language model fine-tuned for various tasks.
- GPT-3.5 Turbo generates a response text based on the input.
- Display Response:The generated response text is displayed for the user to read.
- The generated response text is displayed for the user to read.
- Speech Synthesis:The response text is sent to ElevenLabs TTS API for text-to-speech synthesis.

The API synthesizes the text into an audio file, which can be in .wav or .mp3 format.
- The response text is sent to ElevenLabs TTS API for text-to-speech synthesis.
- The API synthesizes the text into an audio file, which can be in .wav or .mp3 format.
- Play the Response Audio:The synthesized audio file is played back for the user to hear.
- The synthesized audio file is played back for the user to hear.

Installation


Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#installation
Follow these steps to install and set up the project:
- Clone the repository:git clone https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio.git
- Navigate to the project directory:cd AI-Bot-to-Audio
- Create a Python virtual environment:python -m venv venvIf you’re using Python 3.x and the python command doesn’t work, try python3 instead.
- If you’re using Python 3.x and the python command doesn’t work, try python3 instead.
- Activate the virtual environment:On Windows:venv\Scripts\activate

On macOS and Linux:source venv/bin/activate
- On Windows:venv\Scripts\activate
- On macOS and Linux:source venv/bin/activate
- Install dependencies:pip install -r requirements.txt

Running the Project


Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#running-the-project
Follow these steps to run the project:
- Create a .env file:Copy the content from the .env.example file and create a new .env file in the project directory. Populate the .env file with the necessary API keys and configuration variables.
- Copy the content from the .env.example file and create a new .env file in the project directory. Populate the .env file with the necessary API keys and configuration variables.
- Run the frontend:python display.py
- Run the backend application:python main.py
- Start the conversation:Once both the frontend and backend are running, you can start speaking into the microphone. The conversation will be displayed on the frontend interface.
- Once both the frontend and backend are running, you can start speaking into the microphone. The conversation will be displayed on the frontend interface.
- Clear conversation and start afresh:If you want to clear the conversation and start a new one, simply click the “New Conversation” button on the frontend interface.
- If you want to clear the conversation and start a new one, simply click the “New Conversation” button on the frontend interface.
- Enjoy your conversation!You’re all set to interact with the project. Have fun chatting!
- You’re all set to interact with the project. Have fun chatting!

File Descriptions


Link: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio#file-descriptions
Here’s a brief description of the files in the project:
- record.py:This file contains functions to record audio input from the user through a microphone.
- This file contains functions to record audio input from the user through a microphone.
- main.py:Contains functions for the main backend operations of the application.

Includes functions for transcribing audio to text using OpenAI Whisper base model (speech to text).

Also contains functions for generating response text from OpenAI GPT model.

Utilizes ElevenLabs API for text-to-speech synthesis and playback of the audio response.
- Contains functions for the main backend operations of the application.
- Includes functions for transcribing audio to text using OpenAI Whisper base model (speech to text).
- Also contains functions for generating response text from OpenAI GPT model.
- Utilizes ElevenLabs API for text-to-speech synthesis and playback of the audio response.
- display.py:Code for the taipy frontend of the application is implemented in this file.

It handles the user interface and interaction with the backend components.
- Code for the taipy frontend of the application is implemented in this file.
- It handles the user interface and interaction with the backend components.

Source codes: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio